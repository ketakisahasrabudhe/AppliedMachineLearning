{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMS Spam Classification - Model Training\n",
    "\n",
    "This notebook trains and evaluates models for SMS spam classification. We will:\n",
    "1. Load the prepared data splits\n",
    "2. Build text feature extraction pipeline (TF-IDF)\n",
    "3. Train and evaluate multiple models\n",
    "4. Compare benchmark models and select the best one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T06:09:27.445161Z",
     "iopub.status.busy": "2026-01-29T06:09:27.444879Z",
     "iopub.status.idle": "2026-01-29T06:09:28.351470Z",
     "shell.execute_reply": "2026-01-29T06:09:28.351030Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "Load the train, validation, and test splits we created in prepare.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T06:09:28.353322Z",
     "iopub.status.busy": "2026-01-29T06:09:28.353149Z",
     "iopub.status.idle": "2026-01-29T06:09:28.362098Z",
     "shell.execute_reply": "2026-01-29T06:09:28.361869Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 3900, Validation: 836, Test: 836\n"
     ]
    }
   ],
   "source": [
    "# load data splits\n",
    "train_df = pd.read_csv('train.csv')\n",
    "val_df = pd.read_csv('validation.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "print(f\"Train: {len(train_df)}, Validation: {len(val_df)}, Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T06:09:28.379290Z",
     "iopub.status.busy": "2026-01-29T06:09:28.379175Z",
     "iopub.status.idle": "2026-01-29T06:09:28.381232Z",
     "shell.execute_reply": "2026-01-29T06:09:28.381007Z"
    }
   },
   "outputs": [],
   "source": [
    "# separate features and labels\n",
    "X_train = train_df['message']\n",
    "y_train = train_df['label']\n",
    "\n",
    "X_val = val_df['message']\n",
    "y_val = val_df['label']\n",
    "\n",
    "X_test = test_df['message']\n",
    "y_test = test_df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Extraction\n",
    "\n",
    "We'll use TF-IDF (Term Frequency-Inverse Document Frequency) to convert text messages into numerical features. This is a standard approach for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T06:09:28.382503Z",
     "iopub.status.busy": "2026-01-29T06:09:28.382405Z",
     "iopub.status.idle": "2026-01-29T06:09:28.416523Z",
     "shell.execute_reply": "2026-01-29T06:09:28.416291Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF features shape: (3900, 5000)\n"
     ]
    }
   ],
   "source": [
    "# create TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "\n",
    "# fit on training data and transform all sets\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_val_tfidf = vectorizer.transform(X_val)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"TF-IDF features shape: {X_train_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Functions\n",
    "\n",
    "Define functions to fit, score, and evaluate models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T06:09:28.417641Z",
     "iopub.status.busy": "2026-01-29T06:09:28.417568Z",
     "iopub.status.idle": "2026-01-29T06:09:28.419134Z",
     "shell.execute_reply": "2026-01-29T06:09:28.418917Z"
    }
   },
   "outputs": [],
   "source": [
    "def fit_model(model, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Fit a model on training data.\n",
    "    \"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T06:09:28.420102Z",
     "iopub.status.busy": "2026-01-29T06:09:28.420029Z",
     "iopub.status.idle": "2026-01-29T06:09:28.421459Z",
     "shell.execute_reply": "2026-01-29T06:09:28.421259Z"
    }
   },
   "outputs": [],
   "source": [
    "def score_model(model, X, y):\n",
    "    \"\"\"\n",
    "    Score a model on given data. Returns predictions and probabilities.\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T06:09:28.422478Z",
     "iopub.status.busy": "2026-01-29T06:09:28.422391Z",
     "iopub.status.idle": "2026-01-29T06:09:28.424060Z",
     "shell.execute_reply": "2026-01-29T06:09:28.423848Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_predictions(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Evaluate model predictions. Returns accuracy, precision, recall, and F1.\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred),\n",
    "        'recall': recall_score(y_true, y_pred),\n",
    "        'f1': f1_score(y_true, y_pred)\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T06:09:28.425101Z",
     "iopub.status.busy": "2026-01-29T06:09:28.425034Z",
     "iopub.status.idle": "2026-01-29T06:09:28.426913Z",
     "shell.execute_reply": "2026-01-29T06:09:28.426668Z"
    }
   },
   "outputs": [],
   "source": [
    "def validate_model(model, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Fit on train, score and evaluate on both train and validation.\n",
    "    \"\"\"\n",
    "    # fit on training data\n",
    "    model = fit_model(model, X_train, y_train)\n",
    "    \n",
    "    # score on train and validation\n",
    "    y_train_pred = score_model(model, X_train, y_train)\n",
    "    y_val_pred = score_model(model, X_val, y_val)\n",
    "    \n",
    "    # evaluate on both sets\n",
    "    train_metrics = evaluate_predictions(y_train, y_train_pred)\n",
    "    val_metrics = evaluate_predictions(y_val, y_val_pred)\n",
    "    \n",
    "    return model, train_metrics, val_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T06:09:28.427892Z",
     "iopub.status.busy": "2026-01-29T06:09:28.427831Z",
     "iopub.status.idle": "2026-01-29T06:09:28.429392Z",
     "shell.execute_reply": "2026-01-29T06:09:28.429183Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_metrics(metrics, name=\"\"):\n",
    "    \"\"\"\n",
    "    Print metrics in a readable format.\n",
    "    \"\"\"\n",
    "    print(f\"{name}\")\n",
    "    print(f\"  Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall:    {metrics['recall']:.4f}\")\n",
    "    print(f\"  F1 Score:  {metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train and Validate Models\n",
    "\n",
    "We'll train three benchmark models:\n",
    "1. **Logistic Regression** - simple linear classifier\n",
    "2. **Naive Bayes** - probabilistic classifier good for text\n",
    "3. **Linear SVM** - effective for high-dimensional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T06:09:28.430440Z",
     "iopub.status.busy": "2026-01-29T06:09:28.430378Z",
     "iopub.status.idle": "2026-01-29T06:09:28.444402Z",
     "shell.execute_reply": "2026-01-29T06:09:28.444199Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Results:\n",
      "------------------------------\n",
      "Training:\n",
      "  Accuracy:  0.9695\n",
      "  Precision: 0.9903\n",
      "  Recall:    0.7801\n",
      "  F1 Score:  0.8727\n",
      "\n",
      "Validation:\n",
      "  Accuracy:  0.9617\n",
      "  Precision: 0.9878\n",
      "  Recall:    0.7232\n",
      "  F1 Score:  0.8351\n"
     ]
    }
   ],
   "source": [
    "# train and validate logistic regression\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model, lr_train_metrics, lr_val_metrics = validate_model(\n",
    "    lr_model, X_train_tfidf, y_train, X_val_tfidf, y_val\n",
    ")\n",
    "\n",
    "print(\"Logistic Regression Results:\")\n",
    "print(\"-\" * 30)\n",
    "print_metrics(lr_train_metrics, \"Training:\")\n",
    "print()\n",
    "print_metrics(lr_val_metrics, \"Validation:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good results on both train and validation. The model generalizes well with no significant overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T06:09:28.445573Z",
     "iopub.status.busy": "2026-01-29T06:09:28.445505Z",
     "iopub.status.idle": "2026-01-29T06:09:28.452560Z",
     "shell.execute_reply": "2026-01-29T06:09:28.452343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Results:\n",
      "------------------------------\n",
      "Training:\n",
      "  Accuracy:  0.9851\n",
      "  Precision: 1.0000\n",
      "  Recall:    0.8891\n",
      "  F1 Score:  0.9413\n",
      "\n",
      "Validation:\n",
      "  Accuracy:  0.9773\n",
      "  Precision: 1.0000\n",
      "  Recall:    0.8304\n",
      "  F1 Score:  0.9073\n"
     ]
    }
   ],
   "source": [
    "# train and validate naive bayes\n",
    "nb_model = MultinomialNB()\n",
    "nb_model, nb_train_metrics, nb_val_metrics = validate_model(\n",
    "    nb_model, X_train_tfidf, y_train, X_val_tfidf, y_val\n",
    ")\n",
    "\n",
    "print(\"Naive Bayes Results:\")\n",
    "print(\"-\" * 30)\n",
    "print_metrics(nb_train_metrics, \"Training:\")\n",
    "print()\n",
    "print_metrics(nb_val_metrics, \"Validation:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes also performs well. It's fast and works well with text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T06:09:28.453687Z",
     "iopub.status.busy": "2026-01-29T06:09:28.453622Z",
     "iopub.status.idle": "2026-01-29T06:09:28.462890Z",
     "shell.execute_reply": "2026-01-29T06:09:28.462667Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM Results:\n",
      "------------------------------\n",
      "Training:\n",
      "  Accuracy:  0.9997\n",
      "  Precision: 0.9981\n",
      "  Recall:    1.0000\n",
      "  F1 Score:  0.9990\n",
      "\n",
      "Validation:\n",
      "  Accuracy:  0.9856\n",
      "  Precision: 0.9808\n",
      "  Recall:    0.9107\n",
      "  F1 Score:  0.9444\n"
     ]
    }
   ],
   "source": [
    "# train and validate linear SVM\n",
    "svm_model = LinearSVC(max_iter=1000)\n",
    "svm_model, svm_train_metrics, svm_val_metrics = validate_model(\n",
    "    svm_model, X_train_tfidf, y_train, X_val_tfidf, y_val\n",
    ")\n",
    "\n",
    "print(\"Linear SVM Results:\")\n",
    "print(\"-\" * 30)\n",
    "print_metrics(svm_train_metrics, \"Training:\")\n",
    "print()\n",
    "print_metrics(svm_val_metrics, \"Validation:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM also shows strong performance on this classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Tuning\n",
    "\n",
    "Let's tune the best performing model (Logistic Regression) using grid search on train and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T06:09:28.464013Z",
     "iopub.status.busy": "2026-01-29T06:09:28.463948Z",
     "iopub.status.idle": "2026-01-29T06:09:28.555053Z",
     "shell.execute_reply": "2026-01-29T06:09:28.554798Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 10, 'solver': 'lbfgs'}\n",
      "Best CV F1 score: 0.8923\n"
     ]
    }
   ],
   "source": [
    "# combine train and validation for grid search with cross-validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# define parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'solver': ['lbfgs', 'liblinear']\n",
    "}\n",
    "\n",
    "# grid search on training data\n",
    "grid_search = GridSearchCV(\n",
    "    LogisticRegression(max_iter=1000),\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='f1'\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV F1 score: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T06:09:28.556206Z",
     "iopub.status.busy": "2026-01-29T06:09:28.556132Z",
     "iopub.status.idle": "2026-01-29T06:09:28.560759Z",
     "shell.execute_reply": "2026-01-29T06:09:28.560538Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Logistic Regression - Validation:\n",
      "\n",
      "  Accuracy:  0.9833\n",
      "  Precision: 0.9804\n",
      "  Recall:    0.8929\n",
      "  F1 Score:  0.9346\n"
     ]
    }
   ],
   "source": [
    "# validate the tuned model\n",
    "best_lr = grid_search.best_estimator_\n",
    "\n",
    "y_val_pred = score_model(best_lr, X_val_tfidf, y_val)\n",
    "tuned_val_metrics = evaluate_predictions(y_val, y_val_pred)\n",
    "\n",
    "print(\"Tuned Logistic Regression - Validation:\")\n",
    "print_metrics(tuned_val_metrics, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameter tuning shows similar results, which suggests the default parameters were already reasonable for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Set Evaluation - Final Benchmark\n",
    "\n",
    "Now we evaluate all three models on the held-out test set to select the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T06:09:28.561853Z",
     "iopub.status.busy": "2026-01-29T06:09:28.561789Z",
     "iopub.status.idle": "2026-01-29T06:09:28.570204Z",
     "shell.execute_reply": "2026-01-29T06:09:28.569981Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Results:\n",
      "==================================================\n",
      "\n",
      "Logistic Regression:\n",
      "\n",
      "  Accuracy:  0.9593\n",
      "  Precision: 0.9875\n",
      "  Recall:    0.7054\n",
      "  F1 Score:  0.8229\n",
      "\n",
      "Naive Bayes:\n",
      "\n",
      "  Accuracy:  0.9665\n",
      "  Precision: 1.0000\n",
      "  Recall:    0.7500\n",
      "  F1 Score:  0.8571\n",
      "\n",
      "Linear SVM:\n",
      "\n",
      "  Accuracy:  0.9761\n",
      "  Precision: 0.9792\n",
      "  Recall:    0.8393\n",
      "  F1 Score:  0.9038\n"
     ]
    }
   ],
   "source": [
    "# score all models on test set\n",
    "models = {\n",
    "    'Logistic Regression': lr_model,\n",
    "    'Naive Bayes': nb_model,\n",
    "    'Linear SVM': svm_model\n",
    "}\n",
    "\n",
    "test_results = {}\n",
    "\n",
    "print(\"Test Set Results:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_test_pred = score_model(model, X_test_tfidf, y_test)\n",
    "    metrics = evaluate_predictions(y_test, y_test_pred)\n",
    "    test_results[name] = metrics\n",
    "    print(f\"\\n{name}:\")\n",
    "    print_metrics(metrics, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T06:09:28.571262Z",
     "iopub.status.busy": "2026-01-29T06:09:28.571197Z",
     "iopub.status.idle": "2026-01-29T06:09:28.574300Z",
     "shell.execute_reply": "2026-01-29T06:09:28.574078Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Comparison on Test Set:\n",
      "                     accuracy  precision  recall      f1\n",
      "Logistic Regression    0.9593     0.9875  0.7054  0.8229\n",
      "Naive Bayes            0.9665     1.0000  0.7500  0.8571\n",
      "Linear SVM             0.9761     0.9792  0.8393  0.9038\n"
     ]
    }
   ],
   "source": [
    "# compare models in a table\n",
    "results_df = pd.DataFrame(test_results).T\n",
    "results_df = results_df.round(4)\n",
    "print(\"\\nModel Comparison on Test Set:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Select Best Model\n",
    "\n",
    "For spam classification, we care about:\n",
    "- **High Precision**: Minimize false positives (don't classify ham as spam)\n",
    "- **Good Recall**: Catch most spam messages\n",
    "- **F1 Score**: Balance between precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T06:09:28.575380Z",
     "iopub.status.busy": "2026-01-29T06:09:28.575310Z",
     "iopub.status.idle": "2026-01-29T06:09:28.576989Z",
     "shell.execute_reply": "2026-01-29T06:09:28.576760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model: Linear SVM\n",
      "F1 Score: 0.9038\n"
     ]
    }
   ],
   "source": [
    "# select best model based on F1 score\n",
    "best_model_name = max(test_results, key=lambda x: test_results[x]['f1'])\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"F1 Score: {test_results[best_model_name]['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T06:09:28.577930Z",
     "iopub.status.busy": "2026-01-29T06:09:28.577868Z",
     "iopub.status.idle": "2026-01-29T06:09:28.582129Z",
     "shell.execute_reply": "2026-01-29T06:09:28.581903Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear SVM - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99       724\n",
      "        spam       0.98      0.84      0.90       112\n",
      "\n",
      "    accuracy                           0.98       836\n",
      "   macro avg       0.98      0.92      0.95       836\n",
      "weighted avg       0.98      0.98      0.98       836\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# detailed evaluation of best model\n",
    "y_test_pred = score_model(best_model, X_test_tfidf, y_test)\n",
    "\n",
    "print(f\"\\n{best_model_name} - Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=['ham', 'spam']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T06:09:28.583191Z",
     "iopub.status.busy": "2026-01-29T06:09:28.583126Z",
     "iopub.status.idle": "2026-01-29T06:09:28.585343Z",
     "shell.execute_reply": "2026-01-29T06:09:28.585143Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "              Predicted\n",
      "              Ham   Spam\n",
      "Actual Ham     722     2\n",
      "Actual Spam     18    94\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(f\"              Predicted\")\n",
    "print(f\"              Ham   Spam\")\n",
    "print(f\"Actual Ham    {cm[0][0]:4d}  {cm[0][1]:4d}\")\n",
    "print(f\"Actual Spam   {cm[1][0]:4d}  {cm[1][1]:4d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We trained and evaluated three benchmark models for SMS spam classification:\n",
    "\n",
    "1. **Logistic Regression** - Simple and effective linear model\n",
    "2. **Naive Bayes** - Classic text classification model\n",
    "3. **Linear SVM** - Good for high-dimensional sparse data\n",
    "\n",
    "All models performed well with high accuracy and F1 scores. The best model achieves high precision (important for not blocking legitimate messages) while maintaining good recall (catching spam).\n",
    "\n",
    "This prototype demonstrates that ML-based spam classification is feasible and can achieve good results with simple TF-IDF features and standard classifiers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
